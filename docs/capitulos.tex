% Chapter 1

\chapter{Introduction}
\label{Chap1}

\lhead{Chapter 1. \emph{Introduction}}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=5cm]{img/figures/gaze_estimation_setup.png}
    \caption[Overview of Gaze-Estimation setups]{Overview of Gaze-Estimation setups. Recovered from \cite{survey_automatic_gaze_ghosh}}
    \label{gaze_estimation_setup}
 \end{figure}


\section{Gaze-Tracking}
Applications of estimating the human eye gaze direction, or Gaze-Tracking, can be found in multiple study areas such as health care \cite{EyeTrackMedical}, virtual-reality \cite{EyeVR}, and human-computer interaction \cite{Gaze-Controlled-Web}. Devices that utilize gaze-tracking methods for human-computer interaction are often referred to as Eye Trackers. While there is commercial use for Eye Trackers, they have not reached the accuracy needed to allow users with fine-motor disabilities to interact with a computer by using their own eyes as a replacement for a traditional mouse device.

Conventional methods for gaze-tracking require additional devices such as infrared light in order to make the gaze estimations. However, these methods are prone to error in poor light conditions.

Research in computer vision and Deep Learning have allowed improvement in Gaze-Tracking systems by allowing better predictions while also being less intrusive for users with no additional devices required.


\begin{figure}
    \centering
    \includegraphics[scale=0.6]{img/figures/methods_classification.png}
    \caption{Overview classification of Gaze-Tracking methods }
    \label{GEClassification}
 \end{figure}


Gaze-Tracking with computer vision can be achieved with two different type of methods: Model-based methods and Appearance-based methods. These methods will be described in the following sections.

\section{Model-based methods}
Model-based methods, or sometimes referred to as feature-based methods, use the geometric form of the eyes to make the eye gaze prediction.
As seen on \autoref{GEClassification}, these can be interpreted as 3D models. Model-based 3D Gaze-Estimation
methods use 3D eyeball models and estimate the gaze direction using geometric eye features, such as the iris center and the eye corners \cite{general_theory_remote_gaze_3d_model}.

Furthermore, most these methods require an infrared light source (IR) in order to create a refraction in the human eye which is then used to calculate the eye gaze direction by comparing the refraction position with the pupil position. The main drawback of model-based methods is that they can have a lower accuracy with low-resolution images and poor-condition lighting environments. Additionally, all of them have limited working distance.1

Some state-of-the-art commercial products that have adapted these techniques are Tobii Eye Tracker (\url{https://www.tobii.com/}), Eye Tribe (\url{https://imotions.com/hardware/the-eye-tribe-tracker/}) and EyeSee (\url{https://eyesee-research.com/}). 
While these products allow very accurate predictions for the Eye Tracking, they suffer from the previously mentioned model based methods disadvantages while also requiring users to purchase additional expensive equipment. Most Eye-Tracking products also require wearable glasses peripherals that can be intrusive for daily usage. 

With the increase of interest and usage of Gaze-Tracking, there is also a growing need for new Eye-Trackers that are cheap and easy to use.
Gaze-Tracking methods that utilize built-in or external cameras included in almost every computer can be used as a way of making eye tracking more accessible. Appearance-based approaches are beginning to be researched to achieve the previous goal.

\section{Appearance-based methods}
Recently, investigations for appearance based methods with Deep Learning has surfaced which have demonstrated high accuracy while only using on-the-shelf web cameras to capture the human eye appearance hence the method name.

Conventional appearance-based methods used regression in order to learn the mappings from appearance to human gaze. However, since these methods use the appearance of the eye, the main challenges are head motion and subject differences that alter the eye appearance on use.

To overcome the previous challenges, appearance-based Gaze-Estimation with Deep Learning has become a trend in research. Deep Learning based methods can extract high-level abstract gaze features from high-dimensional images, making them more robust and accurate than conventional methods. The mapping between eye-images and gaze direction can be achieved with various regression techniques, such as neural networks, local interpolation, or Gaussian process regression. Convolutional Neural Networks (CNNs) models are commonly used in computer vision tasks for its high accuracy and performance. 
Current state-of-the-art solutions have begun to adopt CNN based architectures, adopting the advantages of appearance-based methods and achieving high accuracy. CNN models used for Gaze-Tracking learn from RGB images which are preprocessed to segment the human eyes in training.

Some of the drawbacks that appearance-based methods have been that they require more images for training than model-based methods, which are not easily retrieved. Additionally, since most appearance-based methods require deep-learning architectures such as Convolutional Neural Networks, they require higher computational power for training. Several works apply CNNs for gaze estimation, as they have shown to outperform traditional model-based approaches \cite{GazeEstimationInTheWild}.

% Chapter 2

\chapter{State-of-the-art}
\label{Chap2}

\lhead{Chapter 2. \emph{State-of-the-art}}

\section{Datasets}
\label{state-art-datasets}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=4cm]{img/figures/datasets.png}
    \caption[Samples from different datasets]{Samples from different datasets. Recovered from \cite{GazeCapture}, \cite{GazeEstimationInTheWild}, \cite{CAVE_0324} and \cite{Zhang2020ETHXGaze} respectively. }
    \label{Datasets}
 \end{figure}

 \begin{table}[h!]
    \caption{Dataset comparison}
    \centering
    \label{tab:dataset}
    \begin{tabulary}{\textwidth}{LCCCCCCCC}
        \hline
        Dataset & Total \# & Size & Subjects \# & Gaze Points \# & Full Face & 2D Gaze & 3D Gaze & Ref \\
        \hline
        GazeCapture 2016 images & 2.4M & 136 GB & 1,474 & 203 & \cmark & \cmark & \xmark & \cite{GazeCapture} \\
        \hline
        MPIIGaze 2015 images & 213K & 2.1 GB & 15 & 203 & \xmark & \cmark & \cmark & \cite{GazeEstimationInTheWild} \\
        \hline
        CAVE 2013 & 6K images & 2.2 GB & 56 & 105 & \cmark & \xmark & \cmark & \cite{CAVE_0324} \\
        \hline
        EyeDiap 2014 & 94 videos & 11.6 TB & 16 & 105 & \cmark & \cmark & \cmark & \cite{CAVE_0324} \\
        \hline
        ETH-XGaze 2020 & 1.1M & 130 GB & 110 & N/A & \cmark & \cmark & \cmark & \cite{Zhang2020ETHXGaze} \\
        \hline
        \end{tabulary}
\end{table}

Until recently, datasets for Gaze-Estimation models training were built manually by taking pictures of different users' faces by placing their head firmly in a stand. Training models require multiple high-quality images, which makes the previous methods slow and not ideal.
To improve the previous methods' problem, multiple data sets containing images for eye tracking have been compiled with crowd founding to train new models.
\autoref{Datasets} represents an overview of different state-of-the-art datasets, while \autoref{tab:dataset} provides an analytical comparison of each of them.

\subsection{GazeCapture}
Krafka et al. built and released the GazeCapture dataset, which crowdfunded a mobile-based data set of approximately 1500 participants from a wide variety of backgrounds captured by an iOS application \cite{GazeCapture}. However, given that this dataset is built with mobile users, training with this dataset may lead to a huge bias towards mobile users which could lead to poor results with laptop or desktop use.
Additionally, the authors provide pre-trained models utilizing the dataset available as open source (\url{https://github.com/CSAILVision/GazeCapture}).

\subsection{MMPIIGaze}
Another state-of-the-art dataset that has been widely used is the MMPIIGaze dataset created by Zhang et al. \cite{GazeEstimationInTheWild}.
The dataset contains a total of 213,659 images from 15 participants. For each participant, there is a varied number of
images ranging from 34,745 to 1,498. The dataset contains multiple features, our target being the 3D gaze target position related to camera.
The dataset is taken from the participant's laptops, contrasting GazeCapture's images taken from mobile devices.

\subsection{Columbia Eye Gaze / CAVE}
The CAVE dataset \cite{CAVE_0324} consists of 5,880 images of 56 different participants. For each participant, there are 5 head poses with 21 gaze directions per head pose. The authors describe the dataset as having more fixed gaze targets than other publicly available gaze datasets. The participants are ethnically diverse and some of them are captured wearing glasses which makes this a strong candidate for learning. The dataset comes with already-segmented eye areas, so segmentation step can be skipped.

\subsection{EyeDiap}
The EYEDIAP dataset was designed to train and evaluate gaze estimation algorithms from RGB and RGB-D data. By using videos instead of pictures,
it manages to be the biggest sized dataset of the state-of-the-art. Additionally, the recording methodology used is designed to systematically include, and isolate,
most of the variables which affect gaze estimation algorithms: head pose and lightning variations.

\subsection{ETH-XGaze}
Finally, the ETH-XGaze dataset \cite{Zhang2020ETHXGaze} consists of over 1,000,000 images of varying gaze under extreme head posers collected from 110 participants While the extreme varied head poses provide good opportunities for learning, for the case of this project the head poses that are not looking into the screen would not be useful so hey must be discarded.



\subsection{Discussion}
As seen on \autoref{tab:dataset}, the dataset with the biggest scale is EyeDiap, by having terabytes of videos with distinct frames of information.
While training with the EyeDiap dataset would create the biggest trained model, the huge size of its collection of frames in the videos would require high computational
power and training time in order to fully train the model. While GazeCapture is also a great candidate, its small number of gaze points 
is insufficient for the purpose of gaze-estimation in a computer screen.


\section{Related Work}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=9cm]{img/figures/gaze-architectures.png}
    \caption{Generalized architectures of state-of-the-art models}
    \label{architectures_comparison}
 \end{figure}

 \begin{table}[h!]
    \caption[2D (cm) and 3D ($^{\circ}$) Gaze-Estimation Benchmark of State-of-the-Art]{2D (cm) and 3D ($^{\circ}$) Gaze-Estimation Benchmark of State-of-the-Art}
    \centering
    \label{tab:architectures}
    \begin{tabulary}{\textwidth}{CCCCCCCCC}
        \hline
        \backslashbox{Models}{Datasets} & MPIIGaze & EyeDiap & GazeCapture (Tablet) & GazeCapture (Phone) & CAVE \\\hline\hline
        Multimodal Convolutional Neural Network & 13.9 $^{\circ}$ & 10.5 $^{\circ}$ & N/A & N/A & N/A \\\hline
        iTracker & 7.67 cm & 10.13 cm & 2.81 cm & 1.86 cm & N/A & \\\hline
        RT-Gene & 5.36 cm & 7.19 cm & N/A & N/A & N/A \\\hline
        Spatial Weights & 4.2 cm & 8.56 cm & N/A & N/A & N/A\\\hline
        Geometric \& Texture-based Networks & N/A & N/A & N/A & N/A & 2.22 $^{\circ}$ \\\hline
        \end{tabulary}
\end{table}

\subsection{Multimodal Convolutional Neural Network}
Zhang et al. proposed a six-layered CNN based on LeNet that takes the eye image as input and combines the head pose in the last fully connected layer of the network \cite{GazeEstimationInTheWild}. The results demonstrated a large performance gap between person-specific training results (RF model) with the proposed CNN model and revealed the potential of appearance-based gaze estimation.

\subsection{Spatial Weight CNN}
After the previous proposal, Zhang et al. proposed a spatial weights CNN based on the AlexNet architecture \cite{alex_net} that takes the full face image as input \cite{written_all_over_your_face}. The spatial weights classify the importance of different facial areas in order to perform the Gaze-Estimation. This mechanism includes three additional 1 x 1 convolutional layer followed by a ReLU activation. Compared to other approaches only use eye regions, this method was demonstrated to be more robust against appearance variation such as head pose as well as illumination. The results achieved an error of ~4.8-6.0$^{\circ}$.

\subsection{iTracker}
Kannan et al. proposed a CNN architecture for an iOS based Gaze-Tracking system called iTracker \cite{iTracker}. The model's large neural network was constructed with multiple smaller neural networks, with four total inputs: a right eye image, a left eye image, a face image, and a face-grid. The face-grid is a 25x25 binary grid that indicates where the user face is placed relatively to the camera view. 

This model achieved 1.66 centimeter error with no calibration needed. The GazeCapture dataset which is described in \autoref{state-art-datasets} was used to train this model.

While achieving good results, this model is biased with mobile usage which makes it unusable for laptop or computer desktop usage. Additionally, the authors describe there being room for improvement for better segmentation of eyes. However, by adjusting the model's face-grid dimensions to simulate webcam dimensions and with a different dataset, the model could potentially be used for desktop/laptop use.

\subsection{Geometric \& Texture-based Networks}
Jyoti et al. proposed an ensemble of networks that uses the full face, left and right eye \cite{jyoti_automatic_2018}.
A Deep Neural Network (DNN) is trained using the face geometric features. Secondly, three CNNs are trained for the texture based features i.e. the segment of the left eye, right eye, and the combined eyes. The model was trained with the publicly available Columbia Eye Gaze and TabletGaze datasets for experiments. When trained with the Columbia Eye Gaza dataset, experiments resulted in an error of 2.22$^{\circ}$. Additionally, the authors emphasize on the importance of choosing
different activation functions for different results by comparing the model outcome with ReLU vs Swish, where the former achieved better performance for geometric network while Swish showed better performance on the rest.

\subsection{RT-GENE}
Fischer et al. proposed a two-stream VGG network that takes the left and right as inputs \cite{fischer_rt-gene_2018}. This approach considers large camera-to-user distances and high variations in head pose found in natural environments. This approach allows automatic annotation of subject's ground truth gaze and head pose regardless of large camera-user distances, achieving an error of 7.7$^{\circ}$. However, the approach setup requires additional devices such as mobile eye-tracking glasses (Kinect v2 RGB-D camera).

\subsection{Discussion}
As seen on \autoref{tab:architectures}, iTracker outperforms every other 2D estimation modeLs when using phone/tablet based datasets. However, when using regular datasets
 taken from computer dimensions, the performance of iTracker drastically becomes lower. On the other hand, Geometric \& Texture-based
networks has the best performance on 3D gaze estimation. 

Many approaches similar to Multimodal Convolutional Networks have been proposed, using different general architectures such as Lenet, Resnet, and others. However,
as of the date of writing this paper, the application of GoogleNet for Gaze-Estimation models have not been proposed. GoogleNet is a deep convolutional
neural network architecture developed by Szegedy et al. in 2014 \cite{going_deeper_googlenet}. GoogleNet has shown to outperform traditional deep convolutional neural networks such as LeNet.
Having a history of outperforming these traditional methods, in this project a GoogleNet model approach for Gaze-Estimation is proposed as a potential alternative to state-of-the-art systems.

\chapter{Development}
\label{Chap3}
\lhead{Chapter 3. \emph{Development}}


\section{Objectives}
For this research, the goal is to design and train new models for Gaze-Estimation that outperform current state-of-the art systems. 
Accuracy can be determined with evaluation metrics described in \autoref{EvaluationMetrics}, while the speed of the model
must be fast enough in order to potentially use this gaze-estimation system in real-time on the average user computer. The
models must be able to query predictions from a standard desktop computer and laptop webcam while handling the possible variance in head pose
and lightning conditions.

\section{Evaluation Metrics}
\label{EvaluationMetrics}
There are two commonly used metrics for performance evaluation in Gaze-Tracking Systems:
Euclidean distance and the angular error. Angular error is used when referring to 3D gaze estimation, while euclidean distance is used for 2D gaze estimation.

Assuming the gaze direction is \boldmath $g \in \mathbb{R}^3$, and the estimated gaze direction is
\boldmath $\hat{g}  \in \mathbb{R}^3$, the angular error can be calculated with the formula:

\begin{equation} \iota _{angular} = \frac{g \cdot  \hat{g} }{ \|g\|\|\hat{g}\| } \end{equation}

For 2D gaze estimation, the actual gaze position is denoted as 
\boldmath $p \in \mathbb{R}^2$ while the estimated gaze position as
\boldmath $\hat{g} \in \mathbb{R}^2$. This would result in the following formula:

\begin{equation} \iota _{Euclidean} = \| p - \hat{p} \|  \end{equation}
For the purpose of this research, the models will use 3D angular error for their predictions.

\section{Proposed Models}
\label{proposed-models}
Given that multiple models are being used for benchmarking and comparison, the following models will be trained:

\begin{itemize}
    \item LeNet [Scratch]
    \item GoogleNet (Inception V3) [Scratch]
    \item GoogleNet (Inception V3) [Pre-Trained]
    \item Resnet-18 [Pre-Trained]
    \item Resnet-34 [Pre-Trained]
    \item Resnet-50 [Pre-Trained]
\end{itemize}
Pre-trained models are also taken into account, since the trade-offs between
training time and accuracy can be analyzed. Pre-trained models are modified so its final classifier layer (usually a fully connected layer or FC) 
is modified for the context of the problem. In this case, the 3D gaze estimation would require an output of two dimensions. Additionally, 
a new convolutional layer can be added to the beginning of the model in order to handle different input sizes that the pre-trained model.

\section{Framework}
In order to build and train the previously proposed models, the PyTorch framework (\url{https://pytorch.org/}) will be used due to its simplicity and flexibility. Additionally, PyTorch 
provides multiple pre-trained models that can be used for transfer learning, including some models proposed in \autoref{proposed-models}. Results will be stored with the Tensorboard
library (\url{https://www.tensorflow.org/tensorboard}) and converted to figures with the Matplotlib and Seaborn libraries.

Given the time constraints of this project, an existing PyTorch implementation of Gaze Estimation was chosen and extended for this project. The project was developed by GitHub user
"Hysts" in 2018, and is available as open source (\url{https://github.com/hysts/pytorch_mpiigaze}. The repository includes scripts, processes, and tools to train different models  
with configuration files. By default, the available datasets in this repository are MPIIGaze and MPIIFaceGaze, the latter being a modified version of MPIIGaze which was described in 
\autoref{Datasets}. Additional models will be added to the repository, as well as some additional code changes such as the implementation of One Cycle scheduler, which will be described in 
\autoref{one-cycle}. The altered version of the repository, or the fork, can be found in the following repository: \url{https://github.com/albertcastaned/pytorch_mpiigaze}. 

The training will be accomplished using an NVIDIA 1060 GTX 3GB GPU, and a AMD Ryzen 5 1600 Six-Core Processor. For faster training, CUDA is configured to use the GPU as the training device.

\section{Dataset preprocessing}
For demonstration purposes, the small sized MPIIGaze dataset will be used for the training of this project's models. As with other Gaze-Estimation datasets, it contains multiple images of the user's
eyes which must be transformed to serve as the input of the models.

\section{Training}
The training of the proposed models will be of a total of 15 epochs or cycles. After each epoch is complete, the training time, accuracy (angular error), loss value, and  
current learning rate will be simultaneously displayed on the terminal and saved as Tensorboard metrics. 

As mentioned on previous sections, some models are pre-trained with just the classifier layer replaced. By doing this, faster training times and better performance are expected from these models,

To handle potential GPU or CPU crashes or application-level errors during training, checkpoints are set to be saved periodically in case there is need for reloading.   

\subsection{One Cycle Learning Rate}
\label{one-cycle}


\begin{figure}
    \centering
    \includegraphics[width=\textwidth,height=5cm]{img/figures/one-cycle.png}
    \caption[One Cycle Learning Rate]{One Cycle Learning Rate. Recovered from \cite{OneCycleFigure}. }
    \label{SampleOneCycle}
 \end{figure}

To further optimize the training of the models and get overall better performance, One Cycle Learning Rate for "super-convergence" 
described by Leslie Smith et al. in 2017 \cite{OneCycle}. The learning rate of the training increases until it reaches
a specified percentage of completion (PoC) and starts decreasing at the same rate until it completes another cycle of length
PoC.

While PyTorch supports One Cycle scheduler for training, the original code retrieved supported default schedulers which requires
additional code for One Cycle scheduler support.

By default, all models were set to have a PoC of 0.3 or 30\% of completion percentage. The behavior of the change in learning rate
is represented by Figure. TODO ADD .

\section{Saving the model}
Once the training has been completed, the models will be saved as .h5 files so that they can be transferred for further learning,
and to use it with a demonstration application by using OpenCV. As a result, the model will predict the angular gaze
of the user, while the OpenCV program will query them at constant intervals of time and displaying this 3D angle on screen.

\chapter{Results}
\label{Chap4}
\lhead{Chapter 4. \emph{Results}}


\chapter{Conclusion}
\label{Chap5}
\lhead{Chapter 5. \emph{Conclusion}}
