
@inproceedings{alex_net,
  title     = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
  volume    = {25},
  url       = {https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  abstract  = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  urldate   = {2022-04-15},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year      = {2012}
}

@inproceedings{CAVE_0324,
  author    = {B.A. Smith and Q. Yin and S.K. Feiner and S.K. Nayar},
  title     = {{G}aze {L}ocking: {P}assive {E}ye {C}ontact {D}etection for {H}uman?{O}bject {I}nteraction},
  booktitle = {ACM Symposium on User Interface Software and Technology (UIST)},
  pages     = {271--280},
  month     = {Oct},
  year      = {2013}
}

@article{cheng_appearance-based_2021,
  title      = {Appearance-based {Gaze} {Estimation} {With} {Deep} {Learning}: {A} {Review} and {Benchmark}},
  shorttitle = {Appearance-based {Gaze} {Estimation} {With} {Deep} {Learning}},
  url        = {http://arxiv.org/abs/2104.12668},
  abstract   = {Gaze estimation reveals where a person is looking. It is an important clue for understanding human intention. The recent development of deep learning has revolutionized many computer vision tasks, the appearance-based gaze estimation is no exception. However, it lacks a guideline for designing deep learning algorithms for gaze estimation tasks. In this paper, we present a comprehensive review of the appearance-based gaze estimation methods with deep learning. We summarize the processing pipeline and discuss these methods from four perspectives: deep feature extraction, deep neural network architecture design, personal calibration as well as device and platform. Since the data pre-processing and post-processing methods are crucial for gaze estimation, we also survey face/eye detection method, data rectification method, 2D/3D gaze conversion method, and gaze origin conversion method. To fairly compare the performance of various gaze estimation approaches, we characterize all the publicly available gaze estimation datasets and collect the code of typical gaze estimation algorithms. We implement these codes and set up a benchmark of converting the results of different methods into the same evaluation metrics. This paper not only serves as a reference to develop deep learning-based gaze estimation methods but also a guideline for future gaze estimation research. Implemented methods and data processing codes are available at http://phi-ai.org/GazeHub.},
  urldate    = {2022-03-31},
  journal    = {arXiv:2104.12668 [cs]},
  author     = {Cheng, Yihua and Wang, Haofei and Bao, Yiwei and Lu, Feng},
  month      = apr,
  year       = {2021},
  note       = {arXiv: 2104.12668},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {arXiv Fulltext PDF:/home/albert/Zotero/storage/BV365UHV/Cheng et al. - 2021 - Appearance-based Gaze Estimation With Deep Learnin.pdf:application/pdf;arXiv.org Snapshot:/home/albert/Zotero/storage/CN64ZCST/2104.html:text/html}
}

@inproceedings{deng_monocular_2017,
  title  = {Monocular {Free}-{Head} {3D} {Gaze} {Tracking} with {Deep} {Learning} and {Geometry} {Constraints}},
  doi    = {10.1109/ICCV.2017.341},
  author = {Deng, Haoping and Zhu, Wangjiang},
  month  = oct,
  year   = {2017},
  pages  = {3162--3171},
  file   = {Full Text PDF:/home/albert/Zotero/storage/W596CQDY/Deng and Zhu - 2017 - Monocular Free-Head 3D Gaze Tracking with Deep Lea.pdf:application/pdf}
}

@inproceedings{EyeTrackMedical,
  author    = {Grillini, Alessandro and Ombelet, Daniel and Soans, Rijul S. and Cornelissen, Frans W.},
  title     = {Towards Using the Spatio-Temporal Properties of Eye Movements to Classify Visual Field Defects},
  year      = {2018},
  isbn      = {9781450357067},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3204493.3204590},
  doi       = {10.1145/3204493.3204590},
  abstract  = {Perimetry---assessment of visual field defects (VFD)---requires patients to be able to maintain a prolonged stable fixation, as well as to provide feedback through motor response. These aspects limit the testable population and often lead to inaccurate results. We hypothesized that different VFD would alter the eye-movements in systematic ways, thus making it possible to infer the presence of VFD by quantifying the spatio-temporal properties of eye movements. We developed a tracking test to record participant's eye-movements while we simulated different gaze-contingent VFD. We tested 50 visually healthy participants and simulated three common scotomas: peripheral loss, central loss and hemifield loss. We quantified spatio-temporal features using cross-correlogram analysis, then applied cross-validation to train a decision tree algorithm to classify the conditions. Our test is faster and more comfortable than standard perimetry and can achieve a classifying accuracy of ∼90\% (True Positive Rate = ∼98\%) with data acquired in less than 2 minutes.},
  booktitle = {Proceedings of the 2018 ACM Symposium on Eye Tracking Research Applications},
  articleno = {38},
  numpages  = {5},
  keywords  = {screening, visual field, features classification, gaze-contingency, simulations, cross-correlation, tracking},
  location  = {Warsaw, Poland},
  series    = {ETRA '18}
}


@inproceedings{EyeVR,
  author = {Outram, Benjamin and Pai, Yun Suen and Person, Tanner and Minamizawa, Kouta and Kunze, Kai},
  year   = {2018},
  month  = {06},
  pages  = {1-5},
  title  = {Anyorbit: orbital navigation in virtual environments with eye-tracking},
  doi    = {10.1145/3204493.3209579}
}

@inproceedings{fischer_rt-gene_2018,
  title      = {{RT}-{GENE}: {Real}-{Time} {Eye} {Gaze} {Estimation} in {Natural} {Environments}},
  shorttitle = {{RT}-{GENE}},
  doi        = {10.1007/978-3-030-01249-6_21},
  abstract   = {In this work, we consider the problem of robust gaze estimation in natural environments. Large camera-to-subject distances and high variations in head pose and eye gaze angles are common in such environments. This leads to two main shortfalls in state-of-the-art methods for gaze estimation: hindered ground truth gaze annotation and diminished gaze estimation accuracy as image resolution decreases with distance. We first record a novel dataset of varied gaze and head pose images in a natural environment, addressing the issue of ground truth annotation by measuring head pose using a motion capture system and eye gaze using mobile eyetracking glasses. We apply semantic image inpainting to the area covered by the glasses to bridge the gap between training and testing images by removing the obtrusiveness of the glasses. We also present a new real-time algorithm involving appearance-based deep convolutional neural networks with increased capacity to cope with the diverse images in the new dataset. Experiments with this network architecture are conducted on a number of diverse eye-gaze datasets including our own, and in cross dataset evaluations. We demonstrate state-of-the-art performance in terms of estimation accuracy in all experiments, and the architecture performs well even on lower resolution images.},
  author     = {Fischer, Tobias and Chang, Hyung and Demiris, Yiannis},
  month      = sep,
  year       = {2018},
  file       = {Full Text PDF:/home/albert/Zotero/storage/KVHQPK5X/Fischer et al. - 2018 - RT-GENE Real-Time Eye Gaze Estimation in Natural .pdf:application/pdf}
}



@inproceedings{Gaze-Controlled-Web,
  author    = {Menges, Raphael and Kumar, Chandan and M\"{u}ller, Daniel and Sengupta, Korok},
  title     = {GazeTheWeb: A Gaze-Controlled Web Browser},
  year      = {2017},
  isbn      = {9781450349000},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3058555.3058582},
  doi       = {10.1145/3058555.3058582},
  abstract  = {Web is essential for most people, and its accessibility should not be limited to conventional input sources like mouse and keyboard. In recent years, eye tracking systems have greatly improved, beginning to play an important role as input medium. In this work, we present GazeTheWeb, a Web browser accessible solely by eye gaze input. It effectively supports all browsing operations like search, navigation and bookmarks. GazeTheWeb is based on a Chromium powered framework, comprising Web extraction to classify interactive elements, and application of gaze interaction paradigms to represent these elements.},
  booktitle = {Proceedings of the 14th International Web for All Conference},
  articleno = {25},
  numpages  = {2},
  keywords  = {Web browser, navigation, eye tracking, Web accessibility, gaze input, eye-controlled interfaces},
  location  = {Perth, Western Australia, Australia},
  series    = {W4A '17}
}

@inproceedings{GazeCapture,
  author    = {K. Krafka and A. Khosla and P. Kellnhofer and H. Kannan and S. Bhandarkar and W. Matusik and A. Torralba},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Eye Tracking for Everyone},
  year      = {2016},
  volume    = {},
  issn      = {1063-6919},
  pages     = {2176-2184},
  keywords  = {gaze tracking;mobile handsets;crowdsourcing;cameras;real-time systems;data models;reliability},
  doi       = {10.1109/CVPR.2016.239},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.239},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  month     = {jun}
}}


@inproceedings{GazeEstimationInTheWild,
  address   = {Boston, MA, USA},
  title     = {Appearance-based gaze estimation in the wild},
  isbn      = {978-1-4673-6964-0},
  url       = {http://ieeexplore.ieee.org/document/7299081/},
  doi       = {10.1109/CVPR.2015.7299081},
  abstract  = {Appearance-based gaze estimation is believed to work well in real-world settings, but existing datasets have been collected under controlled laboratory conditions and methods have been not evaluated across multiple datasets. In this work we study appearance-based gaze estimation in the wild. We present the MPIIGaze dataset that contains 213,659 images we collected from 15 participants during natural everyday laptop use over more than three months. Our dataset is signiﬁcantly more variable than existing ones with respect to appearance and illumination. We also present a method for in-the-wild appearance-based gaze estimation using multimodal convolutional neural networks that signiﬁcantly outperforms state-of-the art methods in the most challenging cross-dataset evaluation. We present an extensive evaluation of several state-of-the-art imagebased gaze estimation algorithms on three current datasets, including our own. This evaluation provides clear insights and allows us to identify key research challenges of gaze estimation in the wild.},
  language  = {en},
  urldate   = {2022-04-06},
  booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  publisher = {IEEE},
  author    = {Zhang, Xucong and Sugano, Yusuke and Fritz, Mario and Bulling, Andreas},
  month     = jun,
  year      = {2015},
  pages     = {4511--4520},
  file      = {Zhang et al. - 2015 - Appearance-based gaze estimation in the wild.pdf:/home/albert/Zotero/storage/3AL79M2C/Zhang et al. - 2015 - Appearance-based gaze estimation in the wild.pdf:application/pdf}
}

@article{general_theory_remote_gaze_3d_model,
  author  = {Guestrin, E.D. and Eizenman, M.},
  journal = {IEEE Transactions on Biomedical Engineering},
  title   = {General theory of remote gaze estimation using the pupil center and corneal reflections},
  year    = {2006},
  volume  = {53},
  number  = {6},
  pages   = {1124-1133},
  doi     = {10.1109/TBME.2005.863952}
}

@phdthesis{iTracker,
  type      = {Thesis},
  title     = {Eye tracking for the {iPhone} using deep learning},
  copyright = {MIT theses are protected by copyright. They may be viewed, downloaded, or printed from this source but further reproduction or distribution in any format is prohibited without written permission.},
  url       = {https://dspace.mit.edu/handle/1721.1/113142},
  abstract  = {Accurate eye trackers on the market today require specialized hardware and are very costly. If eye-tracking could be available for free to anyone with a camera phone, the potential impact could be great. For example, free eye tracking assistive technology could help people with paralysis to regain control of their day-to-day activities, such as sending email. The first part of this thesis describes the software implementation and the current performance metrics of the original iTracker neural network, which was published in the CVPR 2016 paper "Eye Tracking for Everyone." This original iTracker network had a 1.86 centimeter error for eye tracking on the iPhone. The second part of this thesis describes the efforts towards creating an improved neural network with a smaller centimeter error. A new error of 1.66 centimeters (11\% improvement from the previous benchmark) was achieved using ensemble learning with the ResNet10 model with batch normalization.},
  language  = {eng},
  urldate   = {2022-03-27},
  school    = {Massachusetts Institute of Technology},
  author    = {Kannan, Harini D.},
  year      = {2017},
  note      = {Accepted: 2018-01-12T20:59:18Z},
  file      = {Full Text PDF:/home/albert/Zotero/storage/KX27S44Y/Kannan - 2017 - Eye tracking for the iPhone using deep learning.pdf:application/pdf;Snapshot:/home/albert/Zotero/storage/LRY2DGVJ/113142.html:text/html}
}

@inproceedings{jyoti_automatic_2018,
  title  = {Automatic {Eye} {Gaze} {Estimation} using {Geometric} \& {Texture}-based {Networks}},
  doi    = {10.1109/ICPR.2018.8545162},
  author = {Jyoti, Shreyank and Dhall, Abhinav},
  month  = aug,
  year   = {2018},
  pages  = {2474--2479},
  file   = {Full Text PDF:/home/albert/Zotero/storage/S4CMN5HH/Jyoti and Dhall - 2018 - Automatic Eye Gaze Estimation using Geometric & Te.pdf:application/pdf}
}

@article{survey_automatic_gaze_ghosh,
  title      = {Automatic {Gaze} {Analysis}: {A} {Survey} of {Deep} {Learning} based {Approaches}},
  shorttitle = {Automatic {Gaze} {Analysis}},
  url        = {http://arxiv.org/abs/2108.05479},
  abstract   = {Eye gaze analysis is an important research problem in the field of Computer Vision and Human-Computer Interaction. Even with notable progress in the last 10 years, automatic gaze analysis still remains challenging due to the uniqueness of eye appearance, eye-head interplay, occlusion, image quality, and illumination conditions. There are several open questions including what are the important cues to interpret gaze direction in an unconstrained environment without prior knowledge and how to encode them in real-time. We review the progress across a range of gaze analysis tasks and applications to elucidate these fundamental questions; identify effective methods in gaze analysis and provide possible future directions. We analyze recent gaze estimation and segmentation methods, especially in the unsupervised and weakly supervised domain, based on their advantages and reported evaluation metrics. Our analysis shows that the development of a robust and generic gaze analysis method still needs to address real-world challenges such as unconstrained setup and learning with less supervision. We conclude by discussing future research directions for designing a real-world gaze analysis system that can propagate to other domains including Computer Vision, Augmented Reality (AR), Virtual Reality (VR), and Human Computer Interaction (HCI). Project Page: https://github.com/i-am-shreya/EyeGazeSurvey\vphantom{\{}\}\{\vphantom{\}}https://github.com/i-am-shreya/EyeGazeSurvey},
  urldate    = {2022-03-31},
  journal    = {arXiv:2108.05479 [cs]},
  author     = {Ghosh, Shreya and Dhall, Abhinav and Hayat, Munawar and Knibbe, Jarrod and Ji, Qiang},
  month      = aug,
  year       = {2021},
  note       = {arXiv: 2108.05479},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {arXiv Fulltext PDF:/home/albert/Zotero/storage/IPFE5J4B/Ghosh et al. - 2021 - Automatic Gaze Analysis A Survey of Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/home/albert/Zotero/storage/RYJRYIUT/2108.html:text/html}
}

@article{written_all_over_your_face,
  title      = {It's {Written} {All} {Over} {Your} {Face}: {Full}-{Face} {Appearance}-{Based} {Gaze} {Estimation}},
  shorttitle = {It's {Written} {All} {Over} {Your} {Face}},
  url        = {http://arxiv.org/abs/1611.08860},
  abstract   = {Eye gaze is an important non-verbal cue for human affect analysis. Recent gaze estimation work indicated that information from the full face region can benefit performance. Pushing this idea further, we propose an appearance-based method that, in contrast to a long-standing line of work in computer vision, only takes the full face image as input. Our method encodes the face image using a convolutional neural network with spatial weights applied on the feature maps to flexibly suppress or enhance information in different facial regions. Through extensive evaluation, we show that our full-face method significantly outperforms the state of the art for both 2D and 3D gaze estimation, achieving improvements of up to 14.3\% on MPIIGaze and 27.7\% on EYEDIAP for person-independent 3D gaze estimation. We further show that this improvement is consistent across different illumination conditions and gaze directions and particularly pronounced for the most challenging extreme head poses.},
  urldate    = {2022-03-31},
  journal    = {arXiv:1611.08860 [cs]},
  author     = {Zhang, Xucong and Sugano, Yusuke and Fritz, Mario and Bulling, Andreas},
  month      = may,
  year       = {2017},
  note       = {arXiv: 1611.08860},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction},
  file       = {arXiv Fulltext PDF:/home/albert/Zotero/storage/JLI6BFQ4/Zhang et al. - 2017 - It's Written All Over Your Face Full-Face Appeara.pdf:application/pdf;arXiv.org Snapshot:/home/albert/Zotero/storage/ESU9JCRP/1611.html:text/html}
}

@inproceedings{Zhang2020ETHXGaze,
  author    = {Xucong Zhang and Seonwook Park and Thabo Beeler and Derek Bradley and Siyu Tang and Otmar Hilliges},
  title     = {ETH-XGaze: A Large Scale Dataset for Gaze Estimation under Extreme Head Pose and Gaze Variation},
  year      = {2020},
  booktitle = {European Conference on Computer Vision (ECCV)}
}


@article{when_i_look,
	title = {When {I} {Look} into {Your} {Eyes}: {A} {Survey} on {Computer} {Vision} {Contributions} for {Human} {Gaze} {Estimation} and {Tracking}},
	volume = {20},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	shorttitle = {When {I} {Look} into {Your} {Eyes}},
	url = {https://www.mdpi.com/1424-8220/20/13/3739},
	doi = {10.3390/s20133739},
	abstract = {The automatic detection of eye positions, their temporal consistency, and their mapping into a line of sight in the real world (to find where a person is looking at) is reported in the scientific literature as gaze tracking. This has become a very hot topic in the field of computer vision during the last decades, with a surprising and continuously growing number of application fields. A very long journey has been made from the first pioneering works, and this continuous search for more accurate solutions process has been further boosted in the last decade when deep neural networks have revolutionized the whole machine learning area, and gaze tracking as well. In this arena, it is being increasingly useful to find guidance through survey/review articles collecting most relevant works and putting clear pros and cons of existing techniques, also by introducing a precise taxonomy. This kind of manuscripts allows researchers and technicians to choose the better way to move towards their application or scientific goals. In the literature, there exist holistic and specifically technological survey documents (even if not updated), but, unfortunately, there is not an overview discussing how the great advancements in computer vision have impacted gaze tracking. Thus, this work represents an attempt to fill this gap, also introducing a wider point of view that brings to a new taxonomy (extending the consolidated ones) by considering gaze tracking as a more exhaustive task that aims at estimating gaze target from different perspectives: from the eye of the beholder (first-person view), from an external camera framing the beholder’s, from a third-person view looking at the scene where the beholder is placed in, and from an external view independent from the beholder.},
	language = {en},
	number = {13},
	urldate = {2022-04-02},
	journal = {Sensors},
	author = {Cazzato, Dario and Leo, Marco and Distante, Cosimo and Voos, Holger},
	month = jan,
	year = {2020},
	note = {Number: 13
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {gaze tracking, computer vision, gaze estimation, review, survey},
	pages = {3739},
	file = {Full Text PDF:C\:\\Users\\alber\\Zotero\\storage\\XNWN6MUS\\Cazzato et al. - 2020 - When I Look into Your Eyes A Survey on Computer V.pdf:application/pdf;Snapshot:C\:\\Users\\alber\\Zotero\\storage\\ZVVDKTAE\\3739.html:text/html},
}
